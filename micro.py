import wave
import pyaudio
from faster_whisper import WhisperModel

# === ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ ===
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000  # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ 16000 Ğ´Ğ»Ñ Whisper
CHUNK = 1024
RECORD_SECONDS = 5
WAVE_OUTPUT_FILENAME = "audio.wav"

model_size = "tiny"
# === Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½Ğ° ===
p = pyaudio.PyAudio()

stream = p.open(format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK)

print("ğŸ¤ Ğ“Ğ¾Ğ²Ğ¾Ñ€Ğ¸...")

frames = []
for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
    data = stream.read(CHUNK)
    frames.append(data)

print("ğŸ›‘ Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°.")

stream.stop_stream()
stream.close()
p.terminate()

# === Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² .wav ===
wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
wf.setnchannels(CHANNELS)
wf.setsampwidth(p.get_sample_size(FORMAT))
wf.setframerate(RATE)
wf.writeframes(b''.join(frames))
wf.close()

# === Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· faster-whisper ===
model = WhisperModel(model_size, device="cpu", compute_type="int8")

segments, info = model.transcribe(WAVE_OUTPUT_FILENAME, beam_size=5)

print(f"\nğŸŒ Ğ¯Ğ·Ñ‹Ğº: {info.language} (Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {info.language_probability:.2f})")
print("ğŸ“ Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚:")
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")

